{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Regularized Logistic Regression\n",
    "\n",
    "**References:** \n",
    "1. J. Chen, J. Zhou, and J. Ye. [*Integrating Low-Rank and Group-Sparse Structures for Robust Multi-Task Learning*](https://dl.acm.org/doi/abs/10.1145/2020408.2020423) in Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 42-50, August 2011.\n",
    "2. J. Friedman, T. Hastie, and R. Tibshirani. [*A Note on the Group Lasso and a Sparse Group Lasso*](https://arxiv.org/abs/1001.0736), arXiv preprint arXiv:1001.0736, 2010.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In many applications, we are assigned multiple regression tasks that are correlated via a certain structure. Multi-task regression aims to exploit this structure by fitting the regression models simultaneously, so knowledge from one task can be transferred to the other tasks. We consider the multi-task regularized regression problem with $L$ tasks\n",
    "\n",
    "$$\\text{minimize}~ \\phi(W\\theta, Y) + r(\\theta),$$\n",
    "\n",
    "where $\\theta = [\\theta_1~\\ldots~\\theta_L] \\in \\mathbf{R}^{s \\times L}$ is the variable, $W \\in \\mathbf{R}^{p \\times s}$ is the feature matrix shared across tasks, and $Y = [y_1~\\ldots~y_L] \\in \\mathbf{R}^{p \\times L}$ contains the $p$ class labels for each task $l = 1,\\ldots,L$. Here $\\phi:\\mathbf{R}^{p \\times L} \\times \\mathbf{R}^{p \\times L} \\rightarrow \\mathbf{R}$ is the loss function and $r:\\mathbf{R}^{s \\times L} \\rightarrow \\mathbf{R}$ is the regularizer.\n",
    "\n",
    "We focus on the binary classification problem (all entries $Y_{il} \\in \\{-1,1\\}$) and take our loss function to be the logistic loss summed over samples and tasks:\n",
    "\n",
    "$$\\phi(Z,Y)= \\sum_{l=1}^L\\sum_{i=1}^p\\log\\left(1+\\exp(-Y_{il}Z_{il})\\right).$$\n",
    "\n",
    "The task variables are coupled by a low-rank, column-sparse structure, which we capture using the regularizer\n",
    "\n",
    "$$r(\\theta) = \\alpha\\|\\theta\\|_{2,1} + \\beta\\|\\theta\\|_*,$$\n",
    "\n",
    "where $\\|\\theta\\|_{2,1}=\\sum_{l=1}^L\\|\\theta_l\\|_2$ is the group lasso penalty, $\\|\\theta\\|_*$ is the nuclear norm, and $\\alpha>0$ and $\\beta>0$ are parameters.\n",
    "\n",
    "## Reformulate and Solve Problem\n",
    "\n",
    "This problem can be converted to standard form by letting\n",
    "\n",
    "$$f_1(Z) = \\phi(Z,Y), \\quad f_2(\\theta) = \\alpha\\|\\theta\\|_{2,1}, \n",
    "\\quad f_3(\\tilde \\theta) = \\beta\\|\\tilde \\theta\\|_*,$$\n",
    "\n",
    "$$A = \\left[\\begin{array}{cccc}\n",
    "I & -W & 0 \\\\\n",
    "0 & I & -I\n",
    "\\end{array}\\right],\n",
    "\\quad x = \\left[\\begin{array}{c} Z \\\\ \\theta \\\\ \\tilde\\theta \n",
    "\\end{array}\\right],\n",
    "\\quad b = 0.$$\n",
    "\n",
    "We solve an instance with $p = 100, s = 80, L = 3$, and $\\alpha = \\beta = 0.1$. The entries of $W$ are drawn IID from $\\mathcal{N}(0,1)$. To construct $Y$, we generate $\\theta^{\\text{true}} \\in \\mathbf{R}^{s\\times L}$ by drawing its entries IID from $N(0,1)$, then set $Y = \\textbf{sign}(W\\theta^{\\text{true}})$, where the signum function is applied elementwise with the convention $\\textbf{sign}(0)=-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.special import expit\n",
    "from a2dr import a2dr\n",
    "from a2dr.proximal import *\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Problem data.\n",
    "p = 100   # Number of samples.\n",
    "s = 80    # Number of features.\n",
    "L = 3     # Number of tasks.\n",
    "\n",
    "# Parameters.\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "\n",
    "# Feature and class matrices.\n",
    "W = np.random.randn(p,s)\n",
    "theta_true = np.random.randn(s,L)\n",
    "Z_true = W.dot(theta_true)\n",
    "Y = 2*(Z_true > 0) - 1   # Y_{ij} = 1 or -1.\n",
    "\n",
    "# Convert problem to standard form.\n",
    "prox_list = [lambda v, t: prox_logistic(v, t, y = Y.ravel(order='F')),\n",
    "             lambda v, t: prox_group_lasso(v.reshape((p,K), order='F'), t*alpha).ravel(order='F'),\n",
    "             lambda v, t: prox_norm_nuc(v.reshape((p,K), order='F'), t*beta).ravel(order='F')]\n",
    "\n",
    "A_list = [sparse.vstack([sparse.eye(p*L), sparse.csr_matrix((s*L,p*L))]),\n",
    "          sparse.vstack([-sparse.block_diag(L*[X]), sparse.eye(s*L)]),\n",
    "          sparse.vstack([sparse.csr_matrix((p*L,s*L)), -sparse.eye(s*L)])]\n",
    "b = np.zeros(p*L + s*L)\n",
    "\n",
    "# Solve with A2DR.\n",
    "a2dr_result = a2dr(prox_list, A_list, b)\n",
    "a2dr_theta = a2dr_result[\"x_vals\"][-1].reshape((s,L), order='F')\n",
    "\n",
    "# Compute objective.\n",
    "loss = np.sum(-np.log(expit(np.multiply(Y, W.dot(a2dr_theta)))))\n",
    "reg = alpha*np.sum([np.linalg.norm(theta[:,l], 2) for l in range(L)])\n",
    "reg += beta*np.linalg.norm(theta, ord='nuc')\n",
    "a2dr_obj = loss + reg\n",
    "\n",
    "# Print solution.\n",
    "print(\"Objective value:\", a2dr_obj)\n",
    "print(\"Optimal theta:\", a2dr_theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
